# code for BESD model to denoise speech using EEG signals

Official repository for [Speaker-independent Brain Enhanced Speech denoising](https://github.com/LuCeHe/lucehe.github.io/blob/master/files/auditory_attention_inspired_speech_enhancement_ICASSP.pdf) by Maryam Hosseini, Luca Celotti and Ã‰ric Plourde. 
The auditory system is extremely efficient in extracting attended auditory information in the presence of competing speakers. Single-channel speech enhancement algorithms, however, greatly lack this efficacy. In this paper, we propose a novel deep learning method referred to as the Brain Enhanced Speech Denoiser (BESD), that takes advantage of the attended auditory information present in the brain activity of the listener to denoise a multi-talker speech. We use this information to modulate the features learned from the sound and the brain activity, in order to perform speech enhancement. We show that our method successfully enhances a speech mixture, without prior information about the attended speaker, using electroencephalography (EEG) signals recorded from the listener. This makes it a great candidate for realistic applications where no prior information about the attended speaker is available, such as hearing aids or cell phones.  

The dataset used is presented in the work [Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech](https://www.cell.com/action/showPdf?pii=S0960-9822%2818%2930146-5) by M.P. Broderick, A.J. Anderson, G.M. Di Liberto, M.J. Crosse, and E.C. Lalor and is available for download [here](https://datadryad.org/stash/dataset/doi:10.5061/dryad.070jc). We only used the Cocktail Party dataset.

